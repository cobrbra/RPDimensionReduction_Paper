@book{Wainwright2019, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, DOI={10.1017/9781108627771}, publisher={Cambridge University Press}, author={Wainwright, Martin J.}, year={2019}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}


@article{Cook2002,
author = "Cook, R.Dennis and Li, Bing",
doi = "10.1214/aos/1021379861",
fjournal = "Annals of Statistics",
journal = "Ann. Statist.",
month = "04",
number = "2",
pages = "455--474",
publisher = "The Institute of Mathematical Statistics",
title = "Dimension reduction for conditional mean in regression",
url = "https://doi.org/10.1214/aos/1021379861",
volume = "30",
year = "2002"
}

@article{Cook1996,
author = {R. Dennis Cook},
title = {Graphics for Regressions With a Binary Response},
journal = {Journal of the American Statistical Association},
volume = {91},
year = {1996}
}


@article{Cannings2017,
author = {Cannings, Timothy I. and Samworth, Richard J.},
title = {Random-projection ensemble classification},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},

volume = {79},
number = {4},
pages = {959-1035},
keywords = {Aggregation, Classification, High dimensional classification, Random projection},
doi = {10.1111/rssb.12228},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12228},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12228},
abstract = {Summary We introduce a very general method for high dimensional classification, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower dimensional space. In one special case that we study in detail, the random projections are divided into disjoint groups, and within each group we select the projection yielding the smallest estimate of the test error. Our random-projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment. Our theoretical results elucidate the effect on performance of increasing the number of projections. Moreover, under a boundary condition that is implied by the sufficient dimension reduction assumption, we show that the test excess risk of the random-projection ensemble classifier can be controlled by terms that do not depend on the original data dimension and a term that becomes negligible as the number of projections increases. The classifier is also compared empirically with several other popular high dimensional classifiers via an extensive simulation study, which reveals its excellent finite sample performance.},
year = {2017}
}

@inproceedings{Yu2014,
  title={A useful variant of the Davis–Kahan theorem for statisticians},
  author={Yi Yu and Tengyao Wang and Richard J. Samworth},
  year={2014}
}

@article{Taeb2020,
  author={Armeen Taeb and Parikshit Shah and Venkat Chandrasekaran},
  title={{False discovery and its control in low rank estimation}},
  journal={Journal of the Royal Statistical Society Series B},
  year=2020,
  volume={82},
  number={4},
  pages={997-1027},
  month={September},
  keywords={},
  doi={10.1111/rssb.12387},
  abstract={Models specified by low rank matrices are ubiquitous in contemporary applications. In many of these problem domains, the row–column space structure of a low rank matrix carries information about some underlying phenomenon, and it is of interest in inferential settings to evaluate the extent to which the row–column spaces of an estimated low rank matrix signify discoveries about the phenomenon. However, in contrast with variable selection, we lack a formal framework to assess true or false discoveries in low rank estimation; in particular, the key source of difficulty is that the standard notion of a discovery is a discrete notion that is ill suited to the smooth structure underlying low rank matrices. We address this challenge via a geometric reformulation of the concept of a discovery, which then enables a natural definition in the low rank case. We describe and analyse a generalization of the stability selection method of Meinshausen and Bühlmann to control for false discoveries in low rank estimation, and we demonstrate its utility compared with previous approaches via numerical experiments.},
  url={https://ideas.repec.org/a/bla/jorssb/v82y2020i4p997-1027.html}
}

@article{Paulin2016,
author = "Paulin, Daniel and Mackey, Lester and Tropp, Joel A.",
doi = "10.1214/15-AOP1054",
fjournal = "The Annals of Probability",
journal = "Ann. Probab.",
month = "09",
number = "5",
pages = "3431--3473",
publisher = "The Institute of Mathematical Statistics",
title = "Efron–Stein inequalities for random matrices",
url = "https://doi.org/10.1214/15-AOP1054",
volume = "44",
year = "2016"
}

@article{Zou2006,
author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
title = {Sparse Principal Component Analysis},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {2},
pages = {265-286},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X113430},

URL = { 
        https://doi.org/10.1198/106186006X113430
    
},
eprint = { 
        https://doi.org/10.1198/106186006X113430
    
}

}

@article{Suzuki2010,
author = {Suzuki, Taiji and Sugiyama, Masashi},
year = {2010},
month = {01},
pages = {804-811},
title = {Sufficient Dimension Reduction via Squared-loss Mutual Information Estimation.},
volume = {9},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

@article{Yamada2011,
author = {Yamada, Makoto and Niu, Gang and Takagi, Jun and Sugiyama, Masashi},
year = {2011},
month = {03},
pages = {},
title = {Sufficient Component Analysis for Supervised Dimension Reduction}
}

@article{Torkkola2003,
author = {Torkkola, Kari},
year = {2003},
month = {01},
pages = {1415-1438},
title = {Feature Extraction by Non-Parametric Mutual Information Maximization.},
volume = {3},
journal = {Journal of Machine Learning Research},
doi = {10.1162/153244303322753742}
}

@article{Suzuki2008,
author = {Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
year = {2008},
month = {01},
pages = {5-20},
title = {Approximating Mutual Information by Maximum Likelihood Density Ratio Estimation.},
volume = {4},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

@article{Vepakomma2016,
author = {Vepakomma, Praneeth and Tonde, Chetan and Elgammal, Ahmed},
year = {2016},
month = {01},
pages = {},
title = {Supervised Dimensionality Reduction via Distance Correlation Maximization},
volume = {12},
journal = {Electronic Journal of Statistics},
doi = {10.1214/18-EJS1403}
}

@article{Ma2013,
author = {Ma, Yanyuan and Zhu, Liping},
year = {2013},
month = {11},
pages = {},
title = {On estimation efficiency of the central mean subspace},
volume = {76},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/rssb.12044}
}